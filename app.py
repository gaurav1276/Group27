# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CWlfhYEmfVYVbUgyJLpDKBuPMVPdunw2
"""

!pip install datasets
!pip install transformers
!pip install gradio
!pip install torch
!pip install scikit-learn

!pip install rouge_score
!pip install faiss-cpu

import gradio as gr
from sklearn.metrics.pairwise import cosine_similarity

from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle
from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
from datasets import load_dataset

# Load the T5 model & tokenizer
model_name = "dipaktiwari/t5-rag-qa-model"  # You can try "t5-base" for better results
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Move model to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

def generate_answer(question, context, max_length=50):
    # Format input as per T5's requirement
    input_text = f"question: {question}  context: {context}"

    # Tokenize input
    inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move to GPU if available

    # Generate output
    output = model.generate(**inputs, max_length=max_length)

    # Decode and return answer
    return tokenizer.decode(output[0], skip_special_tokens=True)


question = "Who wrote the Harry Potter books?"
question = "how manu copies sold?"
context = "J.K. Rowling is the author of the Harry Potter series, which has sold millions of copies worldwide."

answer = generate_answer(question, context)
print("Answer:", answer)

def processField(data):
    #data_str = data.replace("array", "np.array").replace("dtype=object", "").replace("dtype=int32", "")
    #data = eval(data_str)
    return data['text'][0]

# Load FAISS index and documents
with open("faiss_index.pkl", "rb") as f:
    index = pickle.load(f)

with open("documents.pkl", "rb") as f:
    documents = pickle.load(f)

def generate_rag_answer(question, context, max_length=50):
    # Format input as per T5's requirement
    input_text = f"""question: {question} Answer the question as precise as possible using the provided context. If the answer is
                not contained in the context, say 'answer not available in context'  context: {context}"""


    # Tokenize input
    inputs = tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move to GPU if available

    # Generate output
    output = model.generate(**inputs, max_length=max_length)

    # Decode and return answer
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Load sentence transformer model
embedder = SentenceTransformer('all-MiniLM-L6-v2')

def retrieve_documents(query, top_k=1):
    # Encode the query into a vector
    query_embedding = embedder.encode([query])
    # Reshape the query embedding to a 2D array (1, embedding_dimension)
    #query_embedding = query_embedding.reshape(1, -1)
    #query_embedding = np.array(query_embedding).astype('float32')

    # Search for top-k relevant documents
    distances, indices = index.search(query_embedding, top_k)


    # Fetch corresponding documents
    relevant_docs = [documents[idx] for idx in indices[0]]
    # Check the dimensionality of the index
    print(f"Dimensionality of the FAISS index: {index.d}")
    formatted_result = "\n\n".join(relevant_docs)
    return relevant_docs[0]

"""  **Calculate Matrix:** precision, recall, f1, rouge1, rouge2, rougeL, cosine_similarity_score"""

# Function to calculate Precision, Recall, F1 Score, ROUGE, and Cosine Similarity
def calculate_metrics(reference_answer, generated_answer, relevant_docs, query):
    # Precision, Recall, and F1 (You may need to modify this depending on your setup)
    precision = precision_score([reference_answer], [generated_answer], average='binary', pos_label='1')
    recall = recall_score([reference_answer], [generated_answer], average='binary', pos_label='1')
    f1 = f1_score([reference_answer], [generated_answer], average='binary', pos_label='1')

    # ROUGE score
    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    rouge_scores = rouge_scorer_instance.score(reference_answer, generated_answer)
    rouge1 = rouge_scores['rouge1'].fmeasure
    rouge2 = rouge_scores['rouge2'].fmeasure
    rougeL = rouge_scores['rougeL'].fmeasure

    # Cosine Similarity
    # Create embeddings for the query and relevant documents
    query_embedding = embedder.encode([query])
    docs_embeddings = embedder.encode(relevant_docs)
    cosine_similarities = cosine_similarity(query_embedding, docs_embeddings)

    # Take the maximum cosine similarity
    cosine_similarity_score = np.max(cosine_similarities)

    return precision, recall, f1, rouge1, rouge2, rougeL, cosine_similarity_score

!pip install scikit-learn rouge-score sentence-transformers matplotlib

import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics.pairwise import cosine_similarity

def generate_answer_with_explanation(query):
    # Retrieve relevant documents
    relevant_docs = retrieve_documents(query)

    # Combine retrieved documents as context
    # context = " ".join(relevant_docs)

    answer = generate_rag_answer(query, relevant_docs)


    return answer, relevant_docs

def processField(data):
    #data_str = data.replace("array", "np.array").replace("dtype=object", "").replace("dtype=int32", "")
    #data = eval(data_str)
    return data['text'][0]

val_dataset = load_dataset("squad")['validation']
test_df = val_dataset.to_pandas()


test_df["answer_text"] = test_df["answers"].apply(processField)
test_df.drop(columns=["answers"], inplace=True)

generate_answer_with_explanation("What is an object's mass proportional to at the surface of the Earth?")

interface = gr.Interface(
    fn=generate_answer_with_explanation,
    inputs=gr.Textbox(lines=2, placeholder="Enter your question here..."),
    outputs=[
        gr.Textbox(label="Answer"),
        gr.Textbox(label="Retrieved Context")
    ],
    title="T5 RAG QA Demo",
    description="Ask a question, provide context, and see the generated answer with Precision, Recall, and F1 Score metrics."
)

# Launch Gradio app
interface.launch()